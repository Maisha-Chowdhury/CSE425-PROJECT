# -*- coding: utf-8 -*-
"""MaishaChowdhury_CSE425Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1phylPf1T6GRimpTUzFVcFsmHIdi1cGyV
"""

#from project outline
import torch
import torch . nn as nn
import torch . optim as optim
from torch . utils . data import DataLoader, TensorDataset
import torchvision . transforms as transforms
import torchvision . datasets as datasets

!pip install -q sentence-transformers scikit-learn

import pandas as pd

splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}
df = pd.read_parquet("hf://datasets/fancyzhx/ag_news/" + splits["train"]) #ignoring the warning

import nltk
nltk.download('punkt', force=True)
nltk.download('stopwords', force=True)
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import pandas as pd

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')


stop_words = set(stopwords.words('english'))


def remove_stopwords(text):
    words = word_tokenize(text.lower())
    filtered = [word for word in words if word.isalnum() and word not in stop_words]
    return " ".join(filtered)


df['clean_text'] = df['text'].apply(remove_stopwords)

df = df.drop(columns=["label"])
df.head()

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
#model = SentenceTransformer('all-mpnet-base-v2')   #took more time, no differance in performance
texts = df['clean_text'].tolist()

embeddings = model.encode(texts, show_progress_bar=True)    #using gpu makes it faster

from google.colab import drive
drive.mount('/content/drive')
import pickle

with open('/content/drive/MyDrive/embeddings.pkl', 'wb') as drive_file:
    pickle.dump(embeddings, drive_file)

import numpy as np

with open('/content/drive/MyDrive/embeddings.pkl', 'rb') as file:
    embeddings = pickle.load(file)

embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)

# Defining my Autoencoder
class Autoencoder(nn.Module):
    def __init__(self, input_dim):
        super(Autoencoder, self).__init__()

        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(p=0.2),
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(p=0.3),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Linear(64, 32),
            nn.GELU(),
            nn.Linear(32, 16),
            nn.GELU(),
            nn.Linear(16, 8),   #going lower than 8 worsens the clustering

        )

        self.decoder = nn.Sequential(

            nn.Linear(8,16),
            nn.GELU(),
            nn.Linear(16,32),
            nn.GELU(),
            nn.Linear(32, 64),
            nn.GELU(),
            nn.Linear(64, 128),
            nn.GELU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

    def get_latent(self, x):
        return self.encoder(x)


input_dim = embeddings.shape[1]
print("Input dimentions: ",input_dim)
autoencoder = Autoencoder(input_dim=input_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)

dataset = TensorDataset(embeddings_tensor)
loader = DataLoader(dataset, batch_size=64, shuffle=True)


epochs = 15
autoencoder.train()
for epoch in range(epochs):
    total_loss = 0
    for batch in loader:
        x = batch[0]
        optimizer.zero_grad()
        output = autoencoder(x)
        loss = criterion(output, x)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}")


autoencoder.eval()
with torch.no_grad():
    latent_embeddings = autoencoder.get_latent(embeddings_tensor).numpy()


with open('/content/drive/MyDrive/latent_embeddings2.pkl', 'wb') as file:
    pickle.dump(latent_embeddings, file)

print("Latent embeddings saved to Drive.")

import pickle

with open('/content/drive/MyDrive/latent_embeddings2.pkl', 'rb') as file:
    latent_embeddings = pickle.load(file)

from sklearn.cluster import KMeans

k = 4  # 4 clusters because 4 classes
kmeans = KMeans(n_clusters=k, random_state=60)
kmeans_labels = kmeans.fit_predict(latent_embeddings)

df['kmeans_labels'] = kmeans_labels
#DBSCAN gave same label for all
#Hierarchical Clustering crashes session after using all the ram

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
sil_score = silhouette_score(latent_embeddings, kmeans_labels)
ch_score = calinski_harabasz_score(latent_embeddings, kmeans_labels)
db_score = davies_bouldin_score(latent_embeddings, kmeans_labels)

print(f"Silhouette Score:         {sil_score:.4f}")
print(f"Calinski-Harabasz Score:  {ch_score:.4f}")
print(f"Davies-Bouldin Index:     {db_score:.4f}")

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
reduced = pca.fit_transform(latent_embeddings)

plt.figure(figsize=(8, 6))
plt.scatter(reduced[:, 0], reduced[:, 1], c=df['kmeans_labels'], cmap='tab10', s=10)
plt.title("K-Means Clustering (PCA View)")
plt.show()

"""Self Organizing Map(SOM):"""

!pip install minisom

with open('/content/drive/MyDrive/embeddings.pkl', 'rb') as drive_file:
    embeddings = pickle.load(drive_file)


from minisom import MiniSom
import numpy as np


embeddings = np.array(embeddings)


som = MiniSom(x=2, y=2, input_len=embeddings.shape[1], sigma=1.0, learning_rate=0.5)
som.random_weights_init(embeddings)
som.train_random(data=embeddings, num_iteration=1000)


cluster_assignments = []
for emb in embeddings:
    x, y = som.winner(emb)
    cluster_id = x * 2 + y
    cluster_assignments.append(cluster_id)

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

sil_score = silhouette_score(embeddings, cluster_assignments)
ch_score = calinski_harabasz_score(embeddings, cluster_assignments)
db_score = davies_bouldin_score(embeddings, cluster_assignments)

print(f"Silhouette Score:        {sil_score:.4f}")
print(f"Calinski-Harabasz Index: {ch_score:.4f}")
print(f"Davies-Bouldin Index:    {db_score:.4f}")

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt


pca = PCA(n_components=2)
reduced = pca.fit_transform(embeddings)


cluster_assignments = np.array(cluster_assignments)

# Plot
plt.figure(figsize=(8, 6))
plt.scatter(reduced[:, 0], reduced[:, 1], c=cluster_assignments, cmap='tab10', s=10)
plt.title("SOM Clustering (PCA View)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar(label='Cluster ID')
plt.tight_layout()
plt.show()

